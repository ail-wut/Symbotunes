model:
  model_type: music-vae
  params:
    lr: 0.001
    kl_weight: 0.2
    use_teacher_forcing: True
    encoder_config:
      input_size: 321
      z_size: 512
      hidden_size: 2048
      num_layers: 1
    decoder_config:
      num_tokens: 321
      z_size: 512
      conductor_in: 512
      conductor_hidden: 1024
      conductor_num_layers: 2
      lstm_hidden: 1024
      lstm_num_layers: 2
      num_subsequences: 16
      notes_per_subsequence: 16

dataloaders:
  validation:
    name: "lakh"
    root: "_data"
    split: "test"
    download: True
    transforms:
      - midi_tokenizer:
          tokenizer_params:
            pitch_range: [0, 127]
            vocab_size: 130  # Total vocab size: 130 for pitches, note-off, rest + 512 for drum patterns
            max_bar_length: 16  # Each bar has 16 events (16th notes)
            num_bars: 16  # Total number of bars for hierarchical model
            hierarchical: True  # Use hierarchical modeling
            bar_token_count: 16  # Each subsequence corresponds to a single bar
            beat_resolution: 4  # 16th note intervals (4 intervals per beat in 4/4 time)
            note_on_count: 128  # 128 note-on tokens
      - sample_subsequence:
          subsequence_len: 256
      - toksequence_to_tensor:
    
    batch_size: 64
    num_workers: 16
    collate_fn: "pad_collate_single_sequence"

  train:
    - dataset:
        name: "lakh"
        root: "_data"
        split: "train"
        download: True
        transforms:
          - midi_tokenizer:
              tokenizer_params:
                pitch_range: [0, 127]
                vocab_size: 130  # Total vocab size: 130 for pitches, note-off, rest + 512 for drum patterns
                max_bar_length: 16  # Each bar has 16 events (16th notes)
                num_bars: 16  # Total number of bars for hierarchical model
                hierarchical: True  # Use hierarchical modeling
                bar_token_count: 16  # Each subsequence corresponds to a single bar
                beat_resolution: 4  # 16th note intervals (4 intervals per beat in 4/4 time)
                note_on_count: 128  # 128 note-on tokens
          - sample_subsequence:
              subsequence_len: 256
          - toksequence_to_tensor:
        
        batch_size: 64
        num_workers: 16
        collate_fn: "pad_collate_single_sequence"

lightning:
  trainer:
    benchmark: True
    max_steps: 100000
    # accumulate_grad_batches: 8

callbacks:
  - model_checkpoint:
      filename: "{epoch:06}"
      verbose: True
      save_last: True
  - checkpoint_every_n_steps:
      save_step_frequency: 1000
      prefix: "N-Step-Checkpoint"
      use_modelcheckpoint_filename: False
  - setup_callback:
  - cuda_callback:
